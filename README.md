# DSE220x_Machine_Learning_Fundamentals
All Jupyter Assignment Notebooks

Learning objectives
This course is an intensive introduction to the most widely-used machine learning methods. The first goal is to provide a basic intuitive understanding of these techniques: what they are good for, how they work, how they relate to one another, and their strengths and weaknesses. The second goal is to provide a hands-on feel for these methods through experiments with suitable data sets, using Jupyter notebooks. The third goal is to understand machine learning methods at a deeper level by delving into their mathematical underpinnings. This is crucial to being able to adapt and modify existing methods and to creatively combining them.

Overview

Topics:
Taxonomy of prediction problems
Nearest neighbor methods and families of distance functions
Generalization: what it means; overfitting; selecting parameters using cross-validation
Generative modeling for classification, especially using the multivariate Gaussian
Linear regression and its variants
Logistic regression
Optimization: deriving stochastic gradient descent algorithms and testing convexity
Linear classification using the support vector machine
Nonlinear modeling using basis expansion and kernel methods
Decision trees, boosting, and random forests
Methods for flat and hierarchical clustering
Principal component analysis 
Autoencoders, distributed representations, and deep learning

Course Outline:
Week 1: Introduction: nearest neighbor, and a host of prediction problems
Week 2: Probability basics and generative modeling
Week 3: Linear algebra basics, the multivariate Gaussian, and more generative modeling
Week 4: Linear regression and logistic regression
Week 5: Optimization
Week 6: Support vector machines
Week 7: Beyond linear prediction: kernel methods, decision trees, boosting, random forests
Week 8: Clustering
Week 9: Informative projections
Week 10: Deep learning
